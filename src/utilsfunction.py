# -*- coding: utf-8 -*-
"""utilsFunction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j26YQsjlxvggBT1KgqTRTHtXrSLIbXc4
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

most_common = 20000
max_len = 100
epochs = 10
embed_dim = 100
batch_size = 50

def wordToVec():
  #intialize word2vec dictionary
  word2vec = {}
  with open(os.path.join('/content/gdrive/My Drive/glove.6B.100d.txt'),encoding='utf8') as f:
    for line in f:
      values = line.rstrip().rsplit(' ')
      word = values[0]
      vector = np.asarray(values[1:], dtype = 'float32')
      word2vec[word] = vector
  return word2vec

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
def dataPreprocessing(X,y,most_common, max_len,embed_dim):
  labels = list(set(y['airline_sentiment']))
  num_classes = len(labels)
  Xtweets = X['text']
  #data cleaning
  tok = Tokenizer(num_words=most_common, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True)
  tok.fit_on_texts(Xtweets.values)
  sequences = tok.texts_to_sequences(Xtweets.values)
  #print(sequences)

  word_index = tok.word_index
  print('Found %s unique tokens.' % len(word_index))
  X_data = pad_sequences(sequences, maxlen = max_len)

  #working with the labels
  print('\n\n Labels: ',labels)
  print(y.value_counts())
  #replacing categorical values with number
  y['label'] = 0
  y.loc[y['airline_sentiment'] == 'positive','label'] = 1
  y.loc[y['airline_sentiment'] == 'negative','label'] = 2
  print(y.head(2))
  labels = to_categorical(y['label'], num_classes=num_classes)
  print(labels[:3])
  #building an embedding matrix
  word2vec = wordToVec()
  num_of_words = min(most_common,len(word_index)+1)
  #initializing the embedding matrix
  embedding_matrix = np.zeros((num_of_words,embed_dim))
  for word,i in word_index.items():
    if i < num_of_words:
        embedd_vec = word2vec.get(word)
        if embedd_vec is not None:
            embedding_matrix[i] = embedd_vec

  print('Text 1: ',Xtweets[0])
  print('Text 1 converted to word index sequences: ',sequences[0])
  print('Text 1 word to padded index sequence of length 100: \n',X_data[0])

  return embedding_matrix, X_data, labels